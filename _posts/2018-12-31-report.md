---
title: "report 20181231"
categories:
  - report
author_profile: false
---
# Learning
I have finished major topics around Model-Based RL and wrote an article as a reminder. Here is a list.  
![enter image description here](https://lh3.googleusercontent.com/MW0pe_2qqLChO9QMO9SgX-LvCHKvDog5QsVFt1TU08Dj6jbAdgy0sWFj2WyubE6ViF8RnMrVJzM)
![enter image description here](https://lh3.googleusercontent.com/-a1M9BavkUhEz427zDzoGMqCoRV2qai0yLjvANeUrgS2-7PIpj98wTENAtIEZ6VeZcmTDgz0ZRw)

For more details, check [this blog](https://blog.csdn.net/u010909964/article/details/85329751) if you would like to.

Indeed, I have some thoughts of how to base sim2real learning on what we have learned. For example, Guided Policy Search is to have our policy to imitate an optimal control expert (which plans according to a learned model). Suppose the environment changes, normally data (demonstrations) is few as for policy learning, but if we turn to the data for learning a dynamics, we just observe after we execute an action. Here is an idea. We can learn a policy prior on similar tasks, and fit the local model as the environment changes, and let the optimal control expert (e.g. iLQR) give us demonstrations based on the fitted model, and finally finetune the policy prior. This is a naive idea of combining meta-learning with GPS.

However many things are not clear because the problem setting is not specified.
# Papers
Since I have learnt through major concepts of RL, I plan to revisit the missing points in the papers read. However due to time managing issues of myself, I just finished two of them.

These two revolves around meta-imitation learning, and I focus on how they set up the experiments with a robot.
## 1. One-Shot Visual Imitation Learning via Meta-Learning
As a brief recap of this work: This work proposes to combine MAML with imitation learning, so that the robot can exploit past experience on similar tasks to adapt fast to a new task with only one demonstration. 

Besides a plain recast of MAML into the imitation setting, an interesting point about this work is that they initiated to learn with real-world video demonstrations, and hence proposed a two-head structure to avoid the explict action demonstration. 

Recall that the expert action, which is not explicitly provided in a video demo, occurs in the behavioral cloning loss for imitation learning. And recall that in MAML, this loss is used in two gradient update steps during meta-learning while used once during meta-test for adaptation. 

They keep the final conv layer two-head, i.e., two set of parameters. During meta-learning, the two set of params are respectively used as the basis towards which to add gradient. Because there are two set of params, one can be viewed as params in a learned loss (without needing explicit action to form the supervised loss), and the other to output the real action. Then During meta-test, the first head provides signals for the parameters (except the final layer) to update, and integrate with second head to give actions.  

![enter image description here](https://lh3.googleusercontent.com/CL5Npjqww6wxAxpqFP7LBdxqEfxWDhMaHktw5UC3h45u58rsHf9W1m7EAsOn6CHcJphNuLTf7zE)

Besides this theoretical point, I focus on how they set up their experiments.
### simulated reaching
**task**  
reach an object with target color, with two objects with distracting colors.

**inputs** arm configs, end-effector position  
the vision case and no-vision case differ in the representation of state.  
	1) vision case: the above plus an image 
	2) no-vision case: the above plus 2d positions

**outputs** torques for arm joints

**demonstrations** iLQG optimized expert

**meta-dataset** 
Tasks are different in the target color, the demonstrations for a task are different in the positions of the objects (but other things kept the same). 

This means meta-train enables the policy to learn to reach objects (regardless of what color), and meta-test via a demonstration enables the policy to adapt itself to localize an object of a specific color.

In this experiment, learning from raw pixels is not tested - explicit actions are provided by the optimal control expert.

meta-test includes 150 tasks each 10 demos.

### Simulated Pushing
**task** push an object to a target circle, with a distracting object.

**inputs** image, robot configs, end-effector pose.  
_If there is a distractor, how would the robot know which is the target object to push (without any information)?_  
Later on I have my answer: the robot knows which is the target object via the demo.

**expert**   
train a policy via TRPO to give expert actions. While training, the exact input for the expert is not image but accurate positions for the objects.

**meta-dataset**  
The tasks are different in object configs (e.g. mesh shapes) and textures. The demonstrations for a task seem to be different in object positions.

During meta-train, the policy learns roughly how to localize an object and how to push, and during meta-test the policy finetune to know what object to localize.

In this experiments, additional evaluations are added: 1) eliminate explicit expert actions during meta-test, 2) eliminate expert action and robot arm state during meta-test.

### Real-world placing
**task**  
the robot places what is right in hand to a target placing object, with two distractors.

**inputs** image (at each time step)

**demonstrations**   
obtained via human teleoperation through a motion controller and virtual reality headset. I guess the demonstrations obtained would be of the perspective of the robot, which means there is no issue of distribution shift between training demos and testing demos.

**meta-dataset**  
Tasks are different in objects. Demos for a task are different in the positions of the objects.

## One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning
This work furthers the prior work as to adjusting the domain shift. As a breif recap, this work also combines MAML with imitation learning, but scrutinizes a bit into the idea of a loss learned by the network itself, so that a robot can learn from human video demonstrations.

During meta-learning, features from the perception layer and the control layer are concatenated and sent to a specific component of the network as the learned objective, with human demonstrations. The signals appear to aggregate information from human demonstrations that eventually help to output actions that matches robot demonstrations in the second step. To sum up, this learned objective seems to generate signals that guide the perception layers and the control layers towards transforming human demonstrations to robot demonstrations, e.g. identifying the objects, performing what actions.

Besides, the author suggests using temporal convolution in the learned objective to deal with sequential observations, (which I think is important and inspiring)

### exp1  
- placing: place a held object into a container with two distractors 
- pushing: push an object with a distractor
- pick and place: pick an object and place into a target container with two distracting containers. 
 
 **demonstrations**  
 human demos are in the same perspective as robot demos.  
 Pick&Play use RGB-D demos while the other two use RGB.
 
**meta-dataset**  
meta-train tasks are different from meta-test tasks in the target objects.
 
### exp2: demonstrations with large domain shift
 **domain shift in what sense**  
 different room (lighting & background)  
 different camera  
 different perspective  
 
**meta-dataset**  
meta-train tasks are different from meta-test tasks in the target objects, human demonstrators and (unseen) backgrounds.

**results worth noting**  
The performance shows significant drop as to unseen background, and the author suggests background augmentation.

### exp3  
test the generality of the method on a different robot and different robot demonstration.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEzMTgzODM2NjAsMjEwODA4ODYwMV19
-->